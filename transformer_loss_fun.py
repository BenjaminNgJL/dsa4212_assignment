# -*- coding: utf-8 -*-
"""new_base_loss_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jBw0LdqMdWOFi4lvqfNtf_fcEygl_H9B
"""


!git clone https://github.com/BenjaminNgJL/dsa4212_assignment.git

!git checkout simwei

!ls models

import os
import requests
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import numpy as np
import jax
import jax.numpy as jnp
import optax
import time

import flax.linen as nn
from flax.linen import attention as attn

import sys
sys.path.append('/content/dsa4212_assignment')

# local imports
import models.models_with_pe as models


# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/alexxthiery/char_transformer.git
import util.generation as generation

# initialize the jax random key
key = jax.random.key(0)

"""# Load data"""

test_url = "https://raw.githubusercontent.com/alexxthiery/char_transformer/afa30c8b3c2be8a40a96bdffd26da54e874e06de/data/text8_test.txt"
train_url = "https://raw.githubusercontent.com/alexxthiery/char_transformer/afa30c8b3c2be8a40a96bdffd26da54e874e06de/data/text8_train.txt"

# Download test data
test_text = requests.get(test_url).text
train_text = requests.get(train_url).text

print(test_text[:100])  # preview


# print the length of the training text and test text
print(f"Length of training text: {len(train_text):_} characters")
print(f"Length of test text: {len(test_text):_} characters")

# Build vocabulary (lowercase + space + a few punctuations)
char_set = list("abcdefghijklmnopqrstuvwxyz ")
char_to_int = {ch:i for i,ch in enumerate(char_set)}
int_to_char = {i:ch for ch,i in char_to_int.items()}

def encode(s):
    """Encode string to array of integers"""
    ids = [char_to_int[c] for c in s]
    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space

# encode the text
train_text_int = encode(train_text)
test_text_int = encode(test_text)

# sanity check: display a few random characters from the training text
T = 128
for _ in range(5):
    # choose random position in text
    N = np.random.randint(low=0, high=len(train_text)-T)
    print(train_text[N:N+T])
    print()

"""# Create a basic Transformer model"""

def create_train_state(rng, vocab_size, d_model, n_layers, n_heads,
                       max_len, pos_encoding):
    # create a basic Transformer model
    model = models.DecoderOnlyTransformer(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        max_len=max_len,
        pos_encoding=pos_encoding
    )
    # (sinusoidal, learned, rotary)

    # create a dummy input for initialization
    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)
    # pass the dummy input to the model to initialize the parameters
    params = model.init({"params": rng}, dummy)["params"]

    return model, params

# vocab size
vocab_size= len(char_set)

# internal model dimensions
d_model=256

# number of attention heads
n_heads=8

# number of Transformer layers
n_layers= 4

# maximum sequence length
max_len=128

# positional encoding type
pos_encoding = "learned"

model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len, pos_encoding)

# compute the number of parameters
def count_params(params):
    return sum(x.size for x in jax.tree_util.tree_leaves(params))
print(f"Number of parameters: {count_params(params):_}")

# sanity check: create a batch of data & run a forward pass
B, T = 4, 32
batch = jax.random.randint(
    key=key,
    shape=(B, T), minval=0, maxval=len(char_set))
logits = model.apply({"params": params}, batch)

print("batch shape:", batch.shape)  # (B, T)
print("logits shape:", logits.shape)  # (B, T, vocab_size)

"""# Loss function"""

# 1) cross-entropy
def loss_ce(logits, targets):
    """
    Standard cross-entropy loss with accuracy metrics.
    """
    vocab = logits.shape[-1]
    flat_logits  = logits.reshape(-1, vocab)   # (N, V)
    flat_targets = targets.reshape(-1)         # (N,)

    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)
    loss = per_pos.mean()

    preds = jnp.argmax(logits, axis=-1)  # (B, T)
    is_match = preds == targets

    acc_all  = jnp.mean(is_match.astype(jnp.float32))
    acc_last = jnp.mean(is_match.astype(jnp.float32)[:, -1])

    return loss, {"loss": loss, "acc": acc_all, "acc_last": acc_last}


# 2) Label-smoothed cross-entropy
def loss_label_smooth(logits, targets, epsilon=0.1):
    """Compute label-smoothed cross-entropy loss and accuracy."""
    vocab = logits.shape[-1]

    log_probs = jax.nn.log_softmax(logits, axis=-1)           # (B, T, V)
    targets_onehot = jax.nn.one_hot(targets, vocab)          # (B, T, V)

    smoothed_targets = (1.0 - epsilon) * targets_onehot + epsilon / vocab
    loss = -jnp.sum(smoothed_targets * log_probs, axis=-1).mean()

    preds = jnp.argmax(logits, axis=-1)
    is_match = preds == targets
    acc_all  = jnp.mean(is_match.astype(jnp.float32))
    acc_last = jnp.mean(is_match.astype(jnp.float32)[:, -1])

    return loss, {"loss": loss, "acc": acc_all, "acc_last": acc_last}


# 3) Focal loss (for sequence modelling)
def loss_focal(logits, targets, gamma: float = 2.0, alpha: float | None = None):
    """
    Focal loss for sequence modelling.
    Args:
      logits: (B, T, V)
      targets: (B, T)
    """
    B, T, V = logits.shape

    log_probs = jax.nn.log_softmax(logits, axis=-1)  # (B, T, V)
    probs     = jnp.exp(log_probs)                   # (B, T, V)

    targets_exp = targets[..., None]                 # (B, T, 1)
    p_t     = jnp.take_along_axis(probs,     targets_exp, axis=-1).squeeze(-1)     # (B, T)
    log_p_t = jnp.take_along_axis(log_probs, targets_exp, axis=-1).squeeze(-1)     # (B, T)

    focal_factor = (1.0 - p_t) ** gamma             # (B, T)

    if alpha is not None:
        alpha_t   = jnp.full_like(p_t, alpha)
        focal_loss = -alpha_t * focal_factor * log_p_t
    else:
        focal_loss = -focal_factor * log_p_t

    loss = focal_loss.mean()

    preds = jnp.argmax(logits, axis=-1)
    is_match = preds == targets
    acc_all  = jnp.mean(is_match.astype(jnp.float32))
    acc_last = jnp.mean(is_match.astype(jnp.float32)[:, -1])

    return loss, {"loss": loss, "acc": acc_all, "acc_last": acc_last}

def loss_ls_0_1(logits, targets):
    return loss_label_smooth(logits, targets, epsilon=0.1)

def loss_focal_default(logits, targets):
    return loss_focal(logits, targets, gamma=2.0, alpha=None)

losses_to_test = [
    ("cross_entropy",    loss_ce),
    ("label_smoothing",  loss_ls_0_1),
    ("focal",            loss_focal_default),
]


# Select baseline model for the 3-loss experiment

baseline_model_name  = "256d_8h_baseline"
baseline_model       = model
baseline_params_init = params

"""# Optimization step:"""

@jax.jit
def train_step(params, opt_state, x, y, tx, model, loss_fn):
    """Single optimisation step for the given loss function."""
    def loss_wrapper(params, x, y):
        logits = model.apply({"params": params}, x)
        loss, metrics = loss_fn(logits, y)
        return loss, metrics

    (loss, metrics), grads = jax.value_and_grad(loss_wrapper, has_aux=True)(params, x, y)
    updates, opt_state = tx.update(grads, opt_state, params)
    params = optax.apply_updates(params, updates)
    metrics["loss"] = loss
    return params, opt_state, metrics


"""# Batch creation:"""

# create a batch from the training data
def get_batch(text_int, B, T):
    """Create a random batch of data from text_int.

    Args:
      text_int: 1D array of token ids.
      B: batch size (number of sequences).
      T: sequence length (number of tokens per sequence).

    Returns:
      x: (B, T) int array input tokens.
      y: (B, T) int array target tokens.
    """
    # choose random starting indices for each sequence in the batch
    ix = np.random.randint(0, len(text_int) - T, size=B)
    # inputs are text from i to i+T
    x = np.stack([text_int[i:i+T] for i in ix])
    # targets are text from i+1 to i+T+1
    y = np.stack([text_int[i+1:i+T+1] for i in ix])
    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)

"""# Optimizer creation:"""

# define optax optimizer
learning_rate = 0.001


# Create Adam optimizer (Optax)
tx = optax.adam(learning_rate=learning_rate)
# Initialize optimizer state for current params
opt_state = tx.init(params)
print(f"Initialized optimizer: Adam lr={learning_rate}")

def train_model(model, params, opt_state, train_text_int, test_text_int,
                B, T, tx, model_name, loss_fn, max_time=180):
    """
    Train `model` with the given `loss_fn`.

    loss_fn: callable(logits, targets) -> (loss, metrics_dict)
    metrics_dict must contain keys: 'acc', 'acc_last'.
    """

    # --- Inner jitted train_step that CAPTURES loss_fn & model & tx ---
    @jax.jit
    def train_step_inner(params, opt_state, x, y):
        def loss_wrapper(params, x, y):
            logits = model.apply({"params": params}, x)
            loss, metrics = loss_fn(logits, y)
            return loss, metrics

        (loss, metrics), grads = jax.value_and_grad(loss_wrapper, has_aux=True)(params, x, y)
        updates, opt_state_new = tx.update(grads, opt_state, params)
        params_new = optax.apply_updates(params, updates)
        metrics["loss"] = loss
        return params_new, opt_state_new, metrics

    # --- Training loop (same as before, but calls train_step_inner) ---
    print(f"\nStarting training for {model_name} (max {max_time}s)...")
    time_start = time.time()
    loss_history, time_history = [], []
    loss_test_history, time_test_history = [], []
    it = 0

    while True:
        elapsed = time.time() - time_start
        if elapsed > max_time:
            print(f"Time limit reached ({elapsed:.1f}s). Stopping training.\n")
            break

        x, y = get_batch(train_text_int, B, T)
        params, opt_state, metrics = train_step_inner(params, opt_state, x, y)

        loss_history.append(metrics["loss"])
        time_history.append(elapsed)

        if it % 100 == 0 or (time.time() - time_start) % 30 < 1:
            B_test, T_test = 512, 32
            test_input, test_target = get_batch(test_text_int, B_test, T_test)
            test_logits = model.apply({"params": params}, test_input)
            test_loss, test_metrics = loss_fn(test_logits, test_target)

            loss_test_history.append(test_loss)
            time_test_history.append(elapsed)

            print(
                f"iter {it:05d} | {elapsed:6.1f}s | "
                f"loss(train/test): {metrics['loss']:.4f}/{test_loss:.4f} | "
                f"acc(train/test): {100*metrics['acc']:.1f}%/{100*test_metrics['acc']:.1f}% | "
                f"acc_last(train/test): {100*metrics['acc_last']:.1f}%/{100*test_metrics['acc_last']:.1f}%"
            )
        it += 1

    return params, opt_state, loss_history, time_history, loss_test_history, time_test_history



# ============================================================
# Train with 3 losses
# ============================================================

B, T = 128, 32
max_time = 180
results_loss = []

for loss_name, loss_fn in losses_to_test:
    print("\n==============================")
    print(f"Training {baseline_model_name} with loss = {loss_name}")
    print("==============================")

    params = baseline_params_init
    opt_state = tx.init(params)

    run_name = f"{baseline_model_name}_loss={loss_name}"
    params, opt_state, loss_hist, time_hist, loss_test_hist, time_test_hist = train_model(
        baseline_model,
        params,
        opt_state,
        train_text_int,
        test_text_int,
        B,
        T,
        tx,
        run_name,
        loss_fn,
        max_time=max_time,
    )

    results_loss.append({
        "name": run_name,
        "loss_name": loss_name,
        "params": params,
        "loss_history": loss_hist,
        "time_history": time_hist,
        "test_loss_history": loss_test_hist,
        "test_time_history": time_test_hist,
    })


# ============================================================
# Generate and print text for each of the 3 loss-function models
# ============================================================

B = 1
seed = 42
rng = jax.random.PRNGKey(seed)
prompt = "hello my fri"

# Encode prompt
prompt_int = jnp.array(
    [[char_to_int.get(c, len(char_set)) for c in prompt.lower()[:64]]],
    dtype=jnp.int32
)

gen_len = 1000

print("\n==============================")
print("TEXT GENERATION FOR EACH LOSS FUNCTION")
print("==============================\n")

for entry in results_loss:
    model_name = entry["name"]
    loss_name = entry["loss_name"]
    params = entry["params"]

    print(f"\n===== GENERATED TEXT — {loss_name} =====")

    out_ids = generation.generate_tokens(
        baseline_model,               # <-- USE THE SAME MODEL
        params,
        rng,
        prompt_int,
        gen_len,
        block_size=64,
        temperature=0.7,
        sample=True
    )

    generated_text = ''.join(
        int_to_char.get(int(x), '?') for x in list(out_ids[0])
    )

    print(prompt + generated_text)
    print("=" * 80)

import matplotlib.pyplot as plt

# os.makedirs("plots", exist_ok=True)

plt.figure(figsize=(8, 5))
plt.title("Loss Over Time — Loss Functions")
plt.xlabel("Time (seconds)")
plt.ylabel("Loss")
plt.grid(True)
for r in results_loss:
    plt.plot(r["time_history"], r["loss_history"], label=r["loss_name"], alpha=0.8)
plt.legend()
plt.show()
